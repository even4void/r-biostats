---
title: "Méthodes biostatistiques"
subtitle: "Modèles de régression pour données non gaussiennes"
---

Les paramètres suivants doivent être définis dans R ou RStudio afin de reproduire les analyses présentées dans ce document. Les packages `ggplot2`{.pkg}, `hrbrthemes`{.pkg}, `directlabels`{.pkg}, `cowplot`{.pkg}, `texreg`{.pkg} et `rms`{.pkg} ne font pas partie de la distribution R et doivent être téléchargés au préalable si nécessaire :

```{r, eval = FALSE}
install.packages(c("ggplot2", "hrbrthemes", "directlabels", "cowplot", "texreg", "rms")
```

Les dépendances de ces packages seront installées automatiquement. On supposera également que les instructions R sont exécutées dans un répertoire de travail avec les fichiers de données accessibles dans un sous-répertoire `data/`. Si ce n'est pas le cas, il suffit de créer un sous-répertoire `data/` et d'y enregistrer les fichiers de données, ou de redéfinir les chemins d'accès dans les instructions de lecture des fichiers de données ci-après.

```{r, message = FALSE}
library(hrbrthemes)
library(directlabels)
library(ggfortify)
library(cowplot)
library(texreg)
library(survival)
library(rms)            ## Imports: Hmisc, ggplot2
options(digits = 6, show.signif.stars = FALSE)
theme_set(theme_ipsum(base_size = 11))
```

# Régression logistique

<div class="outline">
Si l'on note $\pi$ la probabilité d'observer l'événement $y=1$ (vs. 0), alors le log odds (transformation \emph{logit}) peut s'exprimer comme une fonction linéaire des paramètres du modèle à $p$ prédicteurs : $$g(x)=\log\left(\frac{\pi}{1-\pi}\right)=\beta_0+\beta_1x_1+\dots+\beta_px_p,$$ et la probabilité prédite s'écrit alors $$P(y=1\mid x_1,x_2,\dots,x_p)=\hat y_i=\frac{\exp(\hat\beta_0+\hat\beta_1x_1+\dots+\hat\beta_px_p)}{1+\exp(\hat\beta_0+\hat\beta_1x_1+\dots+\hat\beta_px_p)}.$$

Dans ce type de modèle, on fait l'hypothèse que $y_i$ suit une distribution binomiale et que les observations sont indépendantes (aucune hypothèse sur la variance qui n'est pas un paramètre dans ce cas de figure). Notons également l'absence de terme d'erreur. L'estimation d'un tel type de modèle se fait par la méthode du maximum de vraisemblance. D'autres fonctions de lien existent (probit, log-log).
</div>

## Chargement des données

Les données `birthwt`{pkg="MASS"} ont déjà été examinées sous l'angle de la régression linéaire en considérant la variable `bwt` (poids du bébé) comme variable à expliquer. Dans les analyses qui suivent, on va s'intéresser à la variable `low` qui représente une indicatrice pour le cas `bwt<2500`.

```{r}
data(birthwt, package = "MASS")
str(birthwt)
```

Voici quelques pré-traitements sur les variables :

```{r}
birthwt$lwt <- birthwt$lwt * 0.45
birthwt$race <- factor(birthwt$race, levels = 1:3, labels = c("white", "black", "other"))
birthwt$smoke <- factor(birthwt$smoke, levels = 0:1, labels = c("non-smoker", "smoker"))
birthwt$ftv[birthwt$ftv > 2] <- 2
```

Voici un résumé de la structure de données en considérant la variable `bwt` et en surlignant les observations pour lesquelles `low=1` :

```{r, echo = FALSE}
p <- ggplot(data = birthwt, aes(x = lwt, y = bwt, color = factor(low))) +
       geom_point() +
       geom_smooth(method = "loess", span = 1.5, se = FALSE, color = grey(.3)) +
       scale_color_manual("", values = c("grey50", "lightcoral")) +
       facet_wrap(~ race, ncol = 3) +
       guides(color = FALSE) +
       labs(x = "Mother weight (kg)", y = "Baby weight (g)")
p
```

## Description des variables

Voici un tableau récapitualitif des variables d'intérêt construit avec `Hmisc`{.pkg} :

```{r}
s <- summary(low ~ age + lwt + race + smoke + ht + ui + ftv,
             data = birthwt, method = "reverse")
print(s, exclude1 = FALSE, npct = "both")
```

Une synthèse graphique de ces mêmes données est proposée ci-dessous :

```{r}
d <- aggregate(low ~ race + cut2(lwt, g = 3) + smoke, data = birthwt, sum)
d$pct <- d$low / nrow(birthwt)
names(d)[2] <- "age"
p <- ggplot(data = d, aes(x = pct, y = age)) +
       geom_line(aes(group = age), color = grey(.7)) +
       geom_point(aes(color = smoke)) +
       scale_color_manual("", values = c("steelblue", "orange")) +
       facet_wrap(~ race, ncol = 3) +
       labs(x = "Proportion weight < 2.5 kg", y = "Mother age")
p
```

## Modélisation

Le modèle de régression logistique s'écrit :

```{r}
fm <- low ~ lwt + race + smoke + ui
m <- glm(fm, data = birthwt, family = binomial)
summary(m)
```

Une fois les paramètres du modèle estimés, il est possible de calculer les intervalles de confiance ou transformer les coefficients de régression sous forme d'odds-ratio :

```{r}
confint(m)
exp(coef(m)["smokesmoker"])
```

Voici les prédictions de ce modèle dans le cas "le plus défavorable" où `smoke=smoker` et `ui=1` :

```{r}
d <- expand.grid(lwt = seq(40, 100), race = factor(levels(birthwt$race)),
                 smoke = "smoker", ui = 1)
d$yhat <- predict(m, d, type = "response")
p <- ggplot(data = d, aes(x = lwt, y = yhat, color = race)) +
       geom_line(aes(group = race), size = 1) +
       scale_color_ipsum() +
       guides(color = FALSE) +
       labs(x = "Mother weight (kg)", y = "Pr(low = 1)", caption = "Predicted response curves")
direct.label(p + aes(label = race), method = "smart.grid")
```

Le package `rms`{.pkg} offre la commande `lrm`{pkg="rms"} pour construire des modèles de régression logistique (cas binomial ou variables ordinales). Cette commande fournit des indices supplémentaires concernant la qualité d'ajustement et la qualité discriminante du modèle, et des procédures de post-estimation identiques à celles décrites dans le cas de la régression linéaire :

```{r}
ddist <- with(birthwt, datadist(lwt, race, smoke, ui))  ## mandatory for Predict with ggplot, see below
options(datadist = "ddist")
m <- lrm(fm, data = birthwt, x = TRUE, y = TRUE)
m
```

```{r}
r <- anova(m)
```

Outre la possibilité d'utiliser `Predict`{pkg="rms"} (P majuscule) pour former des prédictions marginales avec ou sans ajustement, il existe une interface avec le package `ggplot2`{.pkg}. Voici un exemple d'application :

```{r}
ggplot(Predict(m, lwt, race="white", smoke, ui=1), anova = r, pval = TRUE)
```

Enfin, en plus des options disponibles pour la calibration d'un modèle pronostique (`calibrate'`{pkg="rms"}), rms fournit la commande `validate`{pkg="rms"} qui permet d'évaluer la stabilité des paramètres par validation croisée (par défaut, bootstrap) :

```{r}
validate(m)
```

## Cas des données groupées

À titre d'illustration, voici un exemple d'analyse de données médicales pour lesquelles on dispose de la pression systolique (`bpress`) et de la concentration sanguine en cholestérol (`chol`) chez des patients ayant eu ou non un infarctus du myocarde (`hdis`). Le chargement des données ne pose pas de difficulté car les données sont déjà au format R :

```{r}
load("data/hdis.rda")
str(hdis)
```

Voici un résumé de la structure de données :

```{r}
round(xtabs(hdis/total ~ bpress + chol, data = hdis), 2)
```

Pour simplifier l'analyse, on va remplacer les catégories de pression systolique par leur centre de classe et analyser cette variable en mode numérique. La deuxième étape consiste donc à agréger les données sur les différents niveaux de cholestérol :

```{r}
labs <- c(111.5,121.5,131.5,141.5,151.5,161.5,176.5,191.5)
hdis$bpress <- rep(labs, each = 7)
d <- aggregate(cbind(hdis, total) ~ bpress, data = hdis, sum)
```

Voici comment calculer les proportions de cas :

```{r}
d$prop <- d$hdis/d$total
```

Enfin, voici le modèle de régression logistique où, dans le cas des données groupées, il est nécessaire d'indiquer la variable qui code l'événement positif (celui qui nous intéresse) et celle qui code l'événement négatif (son complémentaire) :

```{r}
m <- glm(cbind(hdis, total-hdis) ~ bpress, data = d, family = binomial)
d$yhat <- predict(m, type = "response")
d
```

Les proportions observées et les valeurs prédites sont représentées dans la figure ci-dessous, la fonction `f()` permettant simplement de passer de l'échelle du log odds aux proportions pour le modèle `m` :

```{r}
f <- function(x) 1/(1 + exp(-(coef(m)[1] + coef(m)[2]*x)))
p <- ggplot(data = d, aes(x = bpress, y = hdis/total)) +
       geom_point() +
       stat_function(fun = f, col = "lightcoral", size = 1) +
       labs(x = "Blodd pressure (mm Hg)", y = "Proportion CHD", caption = "Observed vs. fitted proportions")
p
```

# Régression de Poisson

<div class="outline">
La régression de poisson permet de modéliser le log d'un taux ou une variation sous forme d'une combinaison linéaire de $p$ variables explicatives : $$ \log\big(\lambda\mid x_1, x_2, \dots\big)=\lambda_0+\beta_1x_1+\beta_2x_2+\dots, $$ où $\lambda_0$ représente le taux attendu pour un individu pour lequel tous les cofacteurs valent 0. Sur l'échelle naturelle, le modèle prend, comme dans le cas de la régression logistique, une forme multiplicative où $\lambda = \lambda_0 \times \text{RR}_1^{x_1} \times \text{RR}_2^{x_2}  \times \dots$.
</div>


## Chargement des données

La régression de Poisson peut être utilisée dans plusieurs contexte différents : en lien avec la régression logistique (conevrgence de la loi de Poisson vers la loi binomiale), dans le cas du calcul de risque relatif dans les modèles de survie, dans le cadre des procédures de standardisation indirecte, ou plus simplement dans le cas des données de comptage. C'est ce dernier cas qui nous intéresse avec les données [`schoolinf.rda`](data/schoolinf.rda) qui indiquent le nombre de jours après lequel on a diagnostiqué chez des étudiants la survenue d'une maladie infectieuse.

```{r}
load("data/schoolinf.rda")
str(schoolinf)
```

## Description des variables

```{r}
p <- ggplot(data = schoolinf, aes(x = days, y = count)) +
       geom_point() +
       geom_smooth(method = "loess", color = "lightcoral") +
       labs(x = "No. days", y = "No. students diagnosed")
p
```

## Modélisation

Le modèle de Poisson s'écrit comme dans le cas du modèle logistique, en adaptant simplement le type de famille dans la fonciton `glm`{pkg="stats"}:

```{r}
m1 <- glm(count ~ days, data = schoolinf, family = poisson)
summary(m1)
```

On voit que malgré la significativité du coefficient de régression, la variance résiduelle est très élevée (trop proche des degrés de liberté), ce qui suggère un problème de sur-dispersion. Voici une solution pour prendre en compte cette variance non expliquée :

```{r}
m2 <- glm(count ~ days, data = schoolinf, family = quasipoisson)
summary(m2)
```

On voit qu'il s'agit en fait d'un problème de sous-dispersion (le paramètre de dispersion, une fois estimé et non fixé à 1, est < 1).

Voici les prédictions de ce modèle :

```{r}
d <- data.frame(days = seq(0, 150, 2))
d$yhat <- predict(m2, d, type = "response")
p <- ggplot(data = schoolinf, aes(x = days, y = count)) +
       geom_point() +
       geom_line(data = d, aes(x = days, y = yhat), color = "lightcoral", size = 1) +
       labs(x = "No. days", y = "No. students diagnosed")
p
```

# Modèle de Cox

<div class="outline">
Le modèle de régression de Cox est un modèle semi-paramétrique simple dans lequel on modélise le risque ou la fréquence instantanée de l'événement d'intérêt, appelé "hazard", en fonction de variables explicatives sous la forme :
$$ \log\big(h(t\mid x_1,x_2,\dots)\big) = \log\big(h_0(t)\big) + \beta_1x_1 + \beta_2x_2 + \dots. $$ À la différence du modèle de Poisson, le risque de base, $h_0(t)$, peut varier librement d'un individu à l'autre et il n'est pas estimé : on s'intéresse juste au rapport de risques, $\frac{h(t\mid X=x+1)}{h(t\mid X=x)}=\frac{h_0(t)e^{\beta_1(x+1)}}{h_0(t)e^{\beta_1x}}=e^{\beta_1}$.
</div>

## Chargement des données

Le package `survival`{.pkg} permet de traiter le cas de ce type de données, appelées données de survie, pour lesquelles sont associées une variable codant une durée ou une date chronologique et une variable codant un événement binaire (0 = l'événement n'a pas été observé à la dernière date d'observation, 1 = l'événement a été observé ; ici, l'événement est le décès). Voici comment procéder sour R pour définir cette variable composite :

```{r}
data(lung, package = "survival")
lung$sex <- factor(lung$sex, levels = 1:2, labels = c("Male", "Female"))
st <- with(lung, Surv(time=time, event=status))
head(st)
```

## Description des variables

Toutes les commandes R doivent utiliser la variable composite définie par `Surv`{pkg="survival"}, raison pour laquelle il est plus commode de la stocker une fois pour toutes dans une variable, ici `st`. La médiane de survie et son intervalle de confiance pour l'ensemble des individus s'obtient de la manière suivante :

```{r}
s <- survfit(st ~ 1, data=lung)
s
```

Pour obtenir la médiane de survie dans deux groupes, on remplacera le terme d'intercept (`~ 1`) par le facteur correspondant. En réalité, `survfit`{pkg="survival"} permet de construire la table de mortalité, que l'on peut afficher entièrement ou partiellement en appliquant la méthode `summary.survfit`{pkg="survival"} :

```{r}
summary(s, times=seq(1, 200, by=20))
```

Enfin, on peut résumer graphiquement cette table de mortalité à l'aide d'une courbe de Kaplan-Meier (KM) :

```{r}
s <- survfit(st ~ sex, data=lung)
p <- autoplot(s, censor = TRUE, censor.colour = grey(.5)) +
       scale_color_manual("", values = c("steelblue", "orange")) +
       scale_fill_manual("", values = c("steelblue", "orange")) +
       guides(fill = FALSE) +
       theme(legend.position = c(.9, .9)) +
       labs(x = "Followup time (hr.)", y = "Probability survival")
p
```

## Modélisation

Le test du log-rank permettant de tester l'égalité de deux courbes de KM s'obtient avec la commande `survdiff`{pkg="survival"} :

```{r}
survdiff(st ~ sex, data=lung)
```

Quant au modèle de Cox, la syntaxe est la même que dans le cas des autres modèles, à ceci près que la variable réponse est la variable composite définie plus haut, `st` :

```{r}
m1 <- coxph(st ~ sex, data=lung)
m1
```

Il est possible de traiter un cofacteur comme une variable de stratification, de sorte que ses paramètres sont estimés mais ne contribuent pas au degré de liberté du modèle :

```{r}
m2 <- coxph(st ~ sex + strata(age), data=lung)
m2
```

Pour vérifier l'hypothèse de risques proportionnel, il est possible d'utiliser des méthodes graphiques (en examinant la distribution des résidus de Schoenfeld) ou d'utiliser la procédure de test des résidus offerte par la commande `cox.zph`{pkg="survival"}.

```{r}
m3 <- coxph(st ~ sex + age, data=lung)
cox.zph(m3)
```

# Exercices d'application {.tabset}

## Régression logistique

Les données [`clslowbwt.dta`](data/clslowbwt.dta) (format Stata) contiennent des données sur des poids de naissance comparables à celles analysées plus haut. Cette fois, les observations ne sont toutes indépendantes car il y a plusieurs enfants par mère. On ne dispose pas non plus des variables telles que `ht` ou `ui`.

1. Décrire les données, en particulier le poids des bébés (`bwt` et `low`) en fonction des variables d'intérêt (`lwt`, `race`, `smoke`).
2. Estimer les paramètres d'un modèle prédisant `low` en fonction de ces 3 variables.
3. Comparer les résultats (en particulier les erreurs standard) avec ceux obtenus en utilisant un estimateur robuste de la variance (utiliser le package `sandwich`{.pkg} et la fonction `coeftest`{pkg="sandwich"} ou la fonction `robcov`{pkg="rms"} du package `rms`{.pkg}). La variable de cluster sera évidemment l'identifiant des mères.

## Régression logistique pénalisée

Les données [`leukemia.rda`](data/leukemia.rda) contiennent le statut du patient (première colonne) et un ensemble de mesures normalisées d'expression génique (colonne 2 à 3052). L'objectif est de comparer une approche de sélection de variable par tests univariés et par modèle de régerssion pénalisée.

1. Réaliser un test de Student pour chaque variable `X*` et collecter le degré de signification du test. Si le résultat du test est stocké dans une variable, il est possible d'accéder à celui-ci en utilisant l'objet `$p.value`. Afficher la distribution de l'ensemble des p-valeurs sous forme d'histogramme. Combien y'a t-il de tests significatifs avec et sans correction pour les tests multiples ?
2. À l'aide du package glmnet, réaliser une régression logistique avec une pénalisation de type lasso (`alpha=1`). Comparer les variables sélectionnées avec celles retenues en (1).

## Données de survie (1)

Les données [`compliance2.dta`](data/compliance2.dta) (format Stata) contiennent les données d'une étude sur la mortalité de sujets ayant accepté ou non de participer à un programme de dépistage de l'anévrisme de l'aorte abdominale. Au total il y a 555 participants. On souhaite comparer la survie des participants entre les deux groupes (`particp`). Les informations temporelles consistent en une date d'entrée (`randate`) et une date de dernier point (`enddate`) et il faudra donc définir un intervalle `(time=, time2=)` à partir de ces deux variables.

1. Construire la courbe de Kaplan Meier pour les deux groupes de cet échantillon et calculer la valeur du 10e percentile de survie avec son intervalle de confiance à 95 % pour les deux groupes de sujets.
2. Estimer les paramètres d'un modèle de Cox en (a) ajustant ou (b) stratifiant sur l'âge codé en 5 classes (`ranagr`).

## Données de survie (2)

Dans un essai contre placebo sur la cirrhose biliaire, la D-penicillamine (DPCA) a été introduite dans le bras actif sur une cohorte de 312 patients. Au total, 154 patients ont été randomisés dans le bras actif (variable traitement, `rx`, 1=Placebo, 2=DPCA). Un ensemble de données telles que l’âge, des données biologiques et signes cliniques variés incluant le niveau de bilirubine sérique (bilirub) sont disponibles dans le fichier [`pbc.rda`](data/pbc.rda). 1 Le status du patient est enregistré dans la variable `status` (0=vivant, 1=décédé) et la durée de suivi (`years`) représente le temps écoulé en années depuis la date de diagnostic.

Les instructions suivantes permettent de charger les données et d'encoder les
variables catégorielles :

```{r}
load("data/pbc.rda")
pbc$rx <- factor(pbc$rx, levels = 1:2, labels = c("Placebo", "DPCA"))
pbc$sex <- factor(pbc$sex, levels = 0:1, labels = c("M","F"))
```

Voici un tableau descriptif des variables d'intérêt :

```{r}
summary(status ~ rx + sex + cut2(age, g = 4), data = pbc, fun = table)
```

1. Afficher la distribution des durées de suivi des 312 patients, en faisant apparaître distinctement les individus décédés. Calculer le temps médian (en années) de suivi pour chacun des deux groupes de traitement. Combien y’a t-il d’événements positifs au-delà de 10.5 années et quel est le sexe de ces patients ?
2. Calculer la médiane de survie et son intervalle de confiance à 95 % pour chaque groupe de sujets et afficher les courbes de survie correspondantes.
3. Effectuer un test du log-rank en considérant comme prédicteur le facteur `rx`. Comparer avec les résultats obtenus à partir d'un modèle de Cox.
