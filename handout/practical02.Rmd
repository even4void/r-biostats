---
title: "Modèle linéaire et applications"
---

Les paramètres suivants doivent être définis dans R ou RStudio afin de reproduire les analyses présentées dans ce document. Les packages **ggplot2**, **hrbrthemes**, **directlabels**, **cowplot**, **texreg**, **rms**, **mfp**, **multcomp** et **reshape2** ne font pas partie de la distribution R et doivent être téléchargés au préalable (`install.packages(c("ggplot2", "hrbrthemes", "directlabels", "cowplot", "texreg", "rms", "mfp", "multcomp", "reshape2"))`) s'ils ne sont pas déjà installés. Les dépendances de ces packages seront installés automatiquement. On supposera également que les instructions R sont exécutées dans un répertoire de travail avec les fichiers de données accessibles dans un sous-répertoire `data/`. Si ce n'est pas le cas, il suffit de créer un sous-répertoire `data/` et d'y enregistrer les fichiers de données, ou de redéfinir les chemins d'accès dans les instructions de lecture des fichiers de données ci-après.
```{r, message = FALSE}
library(hrbrthemes)
library(directlabels)
library(cowplot)
library(texreg)
library(rms)            ## Hmisc, ggplot2
library(mfp)
library(multcomp)
library(reshape2)
options(digits = 6, show.signif.stars = FALSE)
theme_set(theme_ipsum(base_size = 11))
```


# Régression linéaire simple et multiple

## Chargement des données

Les données utilisées proviennent d'une étude sur le volume expiratoire maximum (en litre par seconde)

```{r}
load("data/fev.rda")
str(FEV)
```

## Description des variables

Un résumé numérique pour l'ensemble des variables peut être obtenu avec `summary()` (on pourrait utiliser la fonction `describe()` de **Hmisc** pour avoir plus de détails) :
```{r}
summary(FEV)
```

On propose quelques pré-traitements pour faciliter les analyses subséquentes :
```{r}
describe(FEV$age)
FEV$age[FEV$age == 3] <- 4         ## low counts in extreme categories
FEV$age[FEV$age == 19] <- 18
FEV$height <- FEV$height * 2.54  ## inches -> centimeters
FEV$lfev <- log(FEV$fev)
```

On va s'intéresser à la relation entre la variable `fev` (voire son log, `lfev`) et les autres variables numériques, `age` et `height`. Voici quelques représentations graphiques préliminaires avec **ggplot2**, dans un premier temps univariées, puis bivariées :
```{r, warning = FALSE}
## switch to long format in order to get a facet variable
d <- melt(FEV[,c("id", "age", "fev", "height")], id.vars = "id")
levels(d$variable) <- c("Age (yr.)", "FEV (l/s)", "Height (cm)")
p <- ggplot(data = d, aes(x = value)) +
       geom_line(stat = "density") +
       geom_rug(size = .5, alpha = .3) +
       facet_wrap(~ variable, ncol = 3, scales = "free")  +
       labs(x = "", y = "Density")
p
```

```{r, warning = FALSE}
## switch to long format in order to get a facet variable
d <- melt(FEV[,c("id", "sex", "smoke", "age", "fev")], measure.vars = 4:5)
levels(d$variable) <- c("Age (yr.)", "FEV (l/s)")
p <- ggplot(data = d, aes(x = sex, y = value)) +
       geom_boxplot() +
       facet_grid(variable ~ smoke, scales = "free_y") +
       labs(x = NULL, y = NULL)
p
```


```{r}
p <- ggplot(data = FEV, aes(x = age, y = fev, color = sex)) +
       geom_point(alpha = 0.5) +
       geom_smooth(method = "loess", se = FALSE) +
       scale_color_manual(values = c("steelblue", "orange")) +
       guides(color = FALSE) +
       labs(x = "Age (yr.)", y = "FEV (l/s)")
p1 <- direct.label(p + aes(label = sex))
p <- ggplot(data = FEV, aes(x = height, y = fev, color = sex)) +
       geom_point(alpha = 0.5) +
       geom_smooth(method = "loess", se = FALSE) +
       scale_color_manual(values = c("steelblue", "orange")) +
       guides(color = FALSE) +
       labs(x = "Height (cm)", y = "FEV (l/s)")
p2 <- direct.label(p + aes(label = sex))
plot_grid(p1, p2)
```

## Modélisation

Un premier modèle que l'on peut considérer est simplement la relation entre FEV et taille sur l'ensemble de l'échantillon :
```{r}
m <- lm(fev ~ height, data = FEV)
summary(m)
```

Les graphiques précédents suggérant toutefois une teandance non linéaire lorsque l'on stratifie sur le sexe, on peut regarder si les conditions de validité du modèle sont bien vérifiées, en particulier en analysant les résidus de ce modèle.
```{r}
yhat <- fitted(m)
rstd <- rstandard(m)
p <- ggplot(data = NULL, aes(x = yhat, y = rstd)) +
       geom_hline(yintercept = 0, linetype = 1, color = grey(.3)) + 
       geom_hline(yintercept = c(-2,2), linetype = 2, color = grey(.3)) + 
       geom_point() +
       geom_smooth(method = "loess", se = FALSE, color = "lightcoral") +
       labs(x = "Fitted values", y = "Residuals")
p
```

On pourrait utiliser une transformation de Box-Cox ($\tfrac{y^{\lambda}-1}{\lambda}$) ou de Tukey ($y^{\lambda}$) pour sélectionner une transformation optimale de la variable réponse pour linéariser la relation, mais on va simplement considérer que la FEV varie linéairement avec le cube de la taille, ce qui revient à modéliser la racine cubique de la FEV, $\sqrt[3]{\text{fev}}$. En pratique, cela reste assez proche de ce que nous donnerait une transformation de Tukey (on trouve $\lambda = 0.263 \approx 1/3$ en utilisant les instructions R `v <- boxcox(m); v$x[which.max(v$y)]`) :
```{r}
m <- lm(I(fev ^ (1/3)) ~ height, data = FEV)
```

```{r, echo = FALSE}
yhat <- fitted(m)
names(yhat) <- FEV$id
rstd <- rstandard(m)
p <- ggplot(data = NULL, aes(x = yhat, y = rstd)) +
       geom_hline(yintercept = 0, linetype = 1, color = grey(.3)) + 
       geom_hline(yintercept = c(-2,2), linetype = 2, color = grey(.3)) + 
       geom_point() +
       annotate("text", x = yhat[abs(rstd) > 3], y = rstd[abs(rstd) > 3], 
                label = names(yhat[abs(rstd) > 3]), size = 3, hjust = -0.5) +
       geom_smooth(method = "loess", se = FALSE, color = "lightcoral") +
       labs(x = "Fitted values", y = "Residuals", caption = ~ "Model considering height" ^3)
p
```

Il reste quelques observations avec des résidus élevés aux deux extrêmes de la distribution des valeurs prédites (les observations avec des résidus supérieurs à 3 en valeur absolue sont annotés avec l'identifiant du sujet). On peut vérifier si ces observations influencent les paramètres estimés à l'aide des mesures d'influence par _jacknife_ fournies par R :
```{r}
influence.measures(m)
```
 
## Autres approches pour la non linéarité

Voici trois approches alternatives permettant de s'affranchir de la stricte linéarité telle qu'assumée dans le modèle de régression linéaire : dans un premier modèle (`m1`), on peut inclure un terme quadratique pour rendre compte du changement de pente apparent pour des tailles supérieures à 160 cm ; sur la même idée, on peut utiliser des polynômes de degré variable, ici d'ordre 3 (`m2`) ; enfin, une approche plus souple par rapport aux polynômes consisterait à utiliser des splines cubiques restreints (`m3`), ici à l'ordre 3 également :  
```{r}
m1 <- lm(fev ~ height + I(height^2), data = FEV)
m2 <- lm(fev ~ pol(height, 3), data = FEV)        ## or poly(height, 3, raw = TRUE)
m3 <- lm(fev ~ rcs(height, 3), data = FEV)
```

Une dernière approche consiste à utiliser des polynömes fractionnaires, l'avantage étant comme dans le cas des splines une plus grande flexibilité et une validation croisée permettant de sélectionner le degré optimal :
```{r}
m4 <- mfp(fev ~ fp(height, df = 4, select = .05), data = FEV)
m4
```

Les prédcitions de ces différents modèles sont affichées dans la figure suivante.

```{r, echo = FALSE}
dd <- data.frame(height = 110:190)
yhat <- cbind.data.frame(dd, y1 = predict(m1, dd), y2 = predict(m2, dd), 
                         y3 = predict(m3, dd), y4 = predict(m4, dd))
p <- ggplot(data = FEV, aes(x = height, y = fev)) +
       geom_point(color = grey(.3), alpha = .5) +
       geom_line(data = melt(yhat, measure.vars = 2:5), aes(x = height, y = value, color = variable), size = 1) +
       scale_color_ipsum(name = "", labels = c("height + height²", "poly(height, 3)", "rcs(height, 3)", "fp(height, 4)")) +
       theme(legend.position = c(0.1, 0.88)) +
       labs(x = "Height (cm)", y = "FEV (l/s)")
p
```

polynômes fractionnaires, splines
rcs



# Régression, ANOVA et test t

## Régression sur variable catégorielle

Avec R, il est important de s'assurer que les variables catégorielles sont bien traitées comme des facteurs, à l'exception des variables binaires codées en 0/1 qui ne posent pas de problème particulier (ni en tant que variable réponse, ni en tant que prédicteur). R utilise des contrastes de traitement par défaut, ce qui signifie que la catégorie de référence à laquelle sont comparés tous les autres niveaux est le premier niveau du facteur. 

Voici une application dans laquelle on modélise la FEV en fonction du statut (fumeur/non-fumeur) :
```{r}
levels(FEV$smoke)
m <- lm(fev ~ smoke, data = FEV)
summary(m)
confint(m)
```

On obtiendra le même résultat, au signe près, avec un test t supposant l'égalité des variances :
```{r}
t.test(fev ~ smoke, data = FEV, var.equal = TRUE)
```

Considérons maintenant l'âge, discrétiser en 4 classes d'effectifs équilibrés, et comparons les résultats d'une régression linéaire sur variable numérique (`age`), d'une régression linéaire sur variable catégorielle (`age4`) et d'une ANOVA :
```{r}
FEV$age4 <- cut2(FEV$age, g = 4)
r <- with(FEV, summarize(fev, age4, smean.sd))
r
r$fev[2:4] - r$fev[1]
m1 <- lm(fev ~ age, data = FEV)
m2 <- lm(fev ~ age4, data = FEV)
m3 <- aov(fev ~ age4, data = FEV)
```

Le modèle `m2` fournit 3 coefficients de régression pour la variable âge à 4 modalités ; chacun de ces coefficients représente la différence de moyenne de la catégorie en question par rapport à la première catégorie (4–9 ans) :
```{r}
summary(m2)
```

On retrouvera facilement les différentiels de moyennes :
```{r}
r <- with(FEV, summarize(fev, age4, smean.sd))
r
```

Voici l'idée schématisée sous forme graphique (les boîtes à moustaches sont alignées sur les centres de classe de la variable `age4`) :

```{r, echo = FALSE}
FEV$age4c <- as.numeric(as.character(cut2(FEV$age, g = 4, levels.mean = TRUE)))
p <- ggplot(data = FEV, aes(x = age, y = fev)) +
       geom_boxplot(aes(x = age4c, y = fev, group = age4c), outlier.size = -1, color = grey(.3), fill = "transparent") +
       geom_jitter(color = grey(.7), alpha = .5, width = .05) +
       geom_smooth(method = "lm", color = "steelblue") +
       labs(x = "Age (yr.)", y = "FEV (l/s)")
p
FEV$age4c <- NULL
```

# Régression multiple

Un autre modèle possible consiste à considérer le statut fumeur et un potentiel facteur de confusion, à savoir l'âge puisqu'en dessous de 9 ans il n'y a pas de fumeurs et qu'au-dessus on a bien deux sous-populations.

```{r, echo = FALSE}
p <- ggplot(data = FEV, aes(x = age, y = fev, color = smoke)) +
       geom_point(alpha = 0.5) +
       geom_smooth(method = "loess", se = FALSE) +
       annotate("rect", xmin = -Inf, ymin = -Inf, xmax = 9, ymax = Inf, fill = grey(.7), alpha = .5) +
       scale_color_manual(values = c("steelblue", "orange")) +
       guides(color = FALSE) +
       labs(x = "Age (yr.)", y = "FEV (l/s)")
direct.label(p + aes(label = smoke))
```

Voici le modèle en question :
```{r}
m1 <- lm(fev ~ smoke, data = FEV)
m2 <- lm(fev ~ age + smoke, data = FEV)
screenreg(list(m1,m2))
```

Comme les modèles sont emboîtés, le test du coefficient pour la variable `age` peut se retrouver par simple comparaison de modèles (par test F ou par test du rapport de vraisemblance) :
```{r}
anova(m1, m2)
```

Et voici le modèle incluant un terme d'interaction entre les variables `age` et `smoke` :
```{r}
m3 <- lm(fev ~ age + smoke + age:smoke, data = FEV)
summary(m3)
```

Le graphique suivant résume la situation, avec des intervalles de confiance à 95 % (non simultanés !) :
```{r}
p <- ggplot(data = FEV, aes(x = age, y = fev, color = smoke)) +
       geom_point(alpha = 0.5) +
       geom_smooth(method = "lm", show.legend = FALSE) +
       scale_color_manual("", values = c("steelblue", "orange")) +
       guides(color = FALSE) +
       labs(x = "Age (yr.)", y = "FEV (l/s)")
direct.label(p + aes(label = smoke))
```


Notons que l'on peut toujours utiliser la fonction `ols()` du package **rms** au lieu de la fonction `lm()` de base. Les sorties sont plus riches, il est possible d'introduire une régularisation de type "ridge" et **rms** fournit des procédure de validation et de calibration du modèle. les commandes de post-estimation (en langage Stata) sont plus intéressantes : on pourrait par exemple calculer les valeurs prédites de `fev` en fonction de `smoke` conditionnellement à un ensemble fixe et pré-spécifié de valeurs de `age`. 

En dernier lieu, si l'on rajoute la variable `height` et `height`^2^, on remarquera que l'interaction `age:smoke` n'est plus significative :
```{r}
m <- ols(fev ~ age + smoke + age:smoke + pol(height, 2), data = FEV, x = TRUE)
m
```

```{r}
d <- expand.grid(age = 9:18, smoke = levels(FEV$smoke), 
                 height = seq(120, 180, by = 10))
yhat <- predict(m, d, se.fit = TRUE)
d <- cbind.data.frame(d, yhat)
head(d)
```

Approche alternative (spécifique au package **rms**) :
```{r}
Predict(m, age = 18, height = seq(160, 180, by = 2), 
        smoke = "current smoker", conf.type = "simult") 
```

# Analyse de covariance

## Chargement des données

## Description des variables

## Modélisation


# Exercices d'application {.tabset}
